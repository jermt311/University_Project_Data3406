{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose/Driving Question\n",
    "\n",
    "In this notebook we use the function obtained from readingData notebook to create cleaned datasets for each of the participants and the Users in our datasets. This means we can just read the cleaned versions each time rather than having to do the preprocessing in each notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages and Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following functions are defined from readingData, but we present them here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_Pacer_data(filename):\n",
    "    #Read in the data\n",
    "    dat = pd.read_csv(filename)\n",
    "    #Select necessary columns\n",
    "    dat = dat[[\"date\",\"steps\"]]\n",
    "    #Extract datetime data\n",
    "    dat[\"datetime\"] = pd.to_datetime(dat[\"date\"], format = '%m/%d/%Y, %H:%M:%S %z')\n",
    "    dat[\"Date\"] = dat[\"datetime\"].dt.date\n",
    "    dat[\"Hour\"] = dat[\"datetime\"].dt.hour\n",
    "    dat[\"Min\"] = dat[\"datetime\"].dt.minute\n",
    "    #Aggregate over the hours\n",
    "    dat = dat.groupby([\"Date\",\"Hour\"])[\"steps\"].agg(\"sum\").reset_index()\n",
    "    #Relabel columns\n",
    "    dat.columns = [[\"Date\", \"Hour\", \"Steps\"]]\n",
    "    \n",
    "    return dat\n",
    "\n",
    "def read_QS_data(filename):\n",
    "    #Read in CSV file\n",
    "    dat = pd.read_csv(filename)\n",
    "    #Extract datetime information\n",
    "    dat[\"Datetime\"] = pd.to_datetime(dat[\"Start\"], format = '%d-%b-%Y %H:%M')\n",
    "    dat[\"Date\"] = dat[\"Datetime\"].dt.date\n",
    "    dat[\"Hour\"] = dat[\"Datetime\"].dt.hour\n",
    "    #Format columns\n",
    "    dat = dat[[\"Date\", \"Hour\", \"Steps (count)\"]]\n",
    "    dat.columns = [\"Date\", \"Hour\", \"Steps\"]\n",
    "    \n",
    "    return dat\n",
    "\n",
    "def read_XML_data(filename):\n",
    "    #Read in XML file\n",
    "    with open(filename, 'r') as xml_file:\n",
    "        input_data = xmltodict.parse(xml_file.read())\n",
    "    #Extract record data from XML\n",
    "    record_list = input_data['HealthData']['Record']\n",
    "    df = pd.DataFrame(record_list)\n",
    "    #Convert dates to datetime objects and steps to numeric\n",
    "    date_format = '%Y-%m-%d %H:%M:%S %z'\n",
    "    df['@startDate'] = pd.to_datetime(df['@startDate'], format = date_format)\n",
    "    df['@endDate'] = pd.to_datetime(df['@endDate'], format = date_format)\n",
    "    df['@value'] = pd.to_numeric(df['@value'])\n",
    "    #Sum up values for each hour\n",
    "    dat = df.resample(\"H\", on=\"@startDate\").sum().reset_index()\n",
    "    #Extract date and hour information, and relabel columns\n",
    "    dat[\"Date\"] = dat[\"@startDate\"].dt.date\n",
    "    dat[\"Hour\"] = dat[\"@startDate\"].dt.hour\n",
    "    dat[\"Steps\"] = dat[\"@value\"]\n",
    "    dat = dat[[\"Date\",\"Hour\",\"Steps\"]]\n",
    "    \n",
    "    return dat\n",
    "\n",
    "def read_CLEAN_data(filename):\n",
    "    #Read in CSV file\n",
    "    dat = pd.read_csv(filename)\n",
    "    #Convert datetimes\n",
    "    dat[\"Date\"] = pd.to_datetime(dat[\"Date\"], format = '%Y-%m-%d').dt.date\n",
    "    \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_step_data(filename, read_type):\n",
    "    read_type = read_type.lower()\n",
    "    if read_type == \"pacer\":\n",
    "        return read_Pacer_data(filename)\n",
    "    elif read_type == \"qsaccess\" or read_type == \"qs\":\n",
    "        return read_QS_data(filename)\n",
    "    elif read_type == \"xml\":\n",
    "        return read_XML_data(filename)\n",
    "    elif read_type == \"clean\" or read_type == \"cleaned\":\n",
    "        return read_CLEAN_data(filename)\n",
    "    else:\n",
    "        raise Exception(\"Not a valid file type to read! Use pacer, qs, xml or clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Datasets\n",
    "\n",
    "For each of the datasets, we import it using the function and then write it to a given csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User 1\n",
    "readfile = \"../../data/Participant_ID_A/User1.csv\"\n",
    "readtype = \"qs\"\n",
    "writefile = \"../../data/cleaned/user1.csv\"\n",
    "read_step_data(readfile, readtype).to_csv(writefile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User 2\n",
    "readfile = \"../../data/Participant_ID_B/User2.csv\"\n",
    "readtype = \"qs\"\n",
    "writefile = \"../../data/cleaned/user2.csv\"\n",
    "read_step_data(readfile, readtype).to_csv(writefile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Participant 1\n",
    "readfile = \"../../data/Participant_ID_01/DetailedSteps_2020_10_24_1932.csv\"\n",
    "readtype = \"pacer\"\n",
    "writefile = \"../../data/cleaned/participant1.csv\"\n",
    "read_step_data(readfile, readtype).to_csv(writefile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Participant 2\n",
    "readfile = \"../../data/Participant_ID_02/export.xml\"\n",
    "readtype = \"xml\"\n",
    "writefile = \"../../data/cleaned/participant2.csv\"\n",
    "read_step_data(readfile, readtype).to_csv(writefile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Participant 3\n",
    "readfile = \"../../data/Participant_ID_03/export.xml\"\n",
    "readtype = \"xml\"\n",
    "writefile = \"../../data/cleaned/participant3.csv\"\n",
    "read_step_data(readfile, readtype).to_csv(writefile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serena\n",
    "readfile = \"../../data/Participant_ID_C/User3.csv\"\n",
    "readtype = \"qs\"\n",
    "writefile = \"../../data/cleaned/serena.csv\"\n",
    "read_step_data(readfile, readtype).to_csv(writefile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Participant 6\n",
    "readfile = \"../../data/Participant_ID_06/DetailedSteps_2020_11_09_1153.csv\"\n",
    "readtype = \"pacer\"\n",
    "writefile = \"../../data/cleaned/participant6.csv\"\n",
    "read_step_data(readfile, readtype).to_csv(writefile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Participant 7\n",
    "readfile = \"../../data/Participant_ID_07/Health Data.csv\"\n",
    "readtype = \"qs\"\n",
    "writefile = \"../../data/cleaned/participant7.csv\"\n",
    "read_step_data(readfile, readtype).to_csv(writefile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
